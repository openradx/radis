x-app: &default-app
  image: radis_dev:latest
  volumes:
    - .:/app
    - /app/.venv
  environment:
    DJANGO_INTERNAL_IPS: ${DJANGO_INTERNAL_IPS:?}
    DJANGO_SETTINGS_MODULE: radis.settings.development
    FORCE_DEBUG_TOOLBAR: ${FORCE_DEBUG_TOOLBAR:-true}
    REMOTE_DEBUGGING_ENABLED: ${REMOTE_DEBUGGING_ENABLED:-false}
    REMOTE_DEBUGGING_PORT: ${REMOTE_DEBUGGING_PORT:-5678}

x-llm: &llm
  hostname: llm.local
  ports:
    - ${LLM_SERVICE_DEV_PORT:-8080}:8080
  environment:
    HF_TOKEN: ${HF_TOKEN:-}
    HTTP_PROXY: ${HTTP_PROXY:-}
    HTTPS_PROXY: ${HTTPS_PROXY:-}
    LLAMA_ARG_MODEL_URL: ${LLAMACPP_MODEL_URL}
    LLAMA_ARG_CTX_SIZE: 8192
    LLAMA_ARG_N_PARALLEL: 2
    LLAMA_ARG_ENDPOINT_METRICS: 1
    LLAMA_ARG_PORT: 8080
    LLAMA_ARG_N_GPU_LAYERS: 99
    NO_PROXY: ${NO_PROXY:-}
  volumes:
    - models_data:/models

services:
  init:
    <<: *default-app
    profiles:
      - never

  web:
    <<: *default-app
    build:
      target: development
    pull_policy: build
    ports:
      - ${WEB_DEV_PORT:-8000}:8000
      - ${REMOTE_DEBUGGING_PORT:-5678}:5678
    command: >
      bash -c "
        wait-for-it -s postgres.local:5432 -t 60 && 
        ./manage.py migrate &&
        ./manage.py create_superuser &&
        ./manage.py create_example_users &&
        ./manage.py create_example_groups &&
        ./manage.py populate_example_reports --lng ${EXAMPLE_REPORTS_LANGUAGE:-en} &&
        ./manage.py runserver 0.0.0.0:8000
      "

  default_worker:
    <<: *default-app
    command: >
      bash -c "
        wait-for-it -s postgres.local:5432 -t 60 &&
        ./manage.py bg_worker -l debug -q default --autoreload
      "

  llm_worker:
    <<: *default-app
    command: >
      bash -c "
        wait-for-it -s postgres.local:5432 -t 60 &&
        ./manage.py bg_worker -l debug -q llm --autoreload
      "

  postgres:
    environment:
      POSTGRES_PASSWORD: postgres
    ports:
      - ${POSTGRES_DEV_PORT:-5432}:5432

  llm_cpu:
    <<: *llm
    image: ghcr.io/ggerganov/llama.cpp:server-b4231
    profiles: [llamaccp_cpu]

  llm_gpu:
    <<: *llm
    image: ghcr.io/ggerganov/llama.cpp:server-cuda-b4231
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles: [llamaccp_gpu]

volumes:
  models_data:
